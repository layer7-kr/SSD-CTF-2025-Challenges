# DSArrrrrgh

# Usage


# Concept
- AES
- Mach-O reversing

# Writeup
바이너리를 실행시켜보면 Enter input: 이 뜨고 맞는입력을 넣으면 correct가 뜨는데 이 correct값을 찾는게 목표입니다.
ida로 열어보면 main이 있습니다
```c
int __fastcall main(int argc, const char **argv, const char **envp)
{
  ssize_t v3; // x0
  int v4; // w9
  __int64 v7; // x8
  __int64 v8; // x9
  __int64 v9; // t2
  __int64 v10; // x8
  unsigned __int64 v11; // x9
  unsigned __int64 v12; // x8
  int v13; // w10
  int v14; // w8
  int v15; // w10
  int v16; // w10
  unsigned __int64 v17; // t2
  int v18; // w10
  __int64 v19; // x8
  unsigned int v20; // w11
  unsigned int v21; // w10
  unsigned __int64 v22; // x19
  const char *v23; // x0
  _BYTE v25[256]; // [xsp+18h] [xbp-128h] BYREF

  printf("Enter input: ");
  fflush(__stdoutp);
  v3 = read(0, v25, 0x100uLL);
  if ( v3 < 1 )
  {
    puts("No input");
    return 1;
  }
  else
  {
    while ( 1 )
    {
      v4 = (unsigned __int8)v25[v3 - 1];
      if ( v4 != 10 && v4 != 13 )
        break;
      if ( (unsigned __int64)v3-- <= 1 )
      {
        v8 = 0xCBF29CE484222325LL;
        goto LABEL_11;
      }
    }
    v7 = 0LL;
    v8 = 0xCBF29CE484222325LL;
    do
      v8 = 0x100000001B3LL * (v8 ^ (unsigned __int8)v25[v7++]);
    while ( v3 != v7 );
LABEL_11:
    v9 = __ROR8__(
           ((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x11) << 8) ^ __ROR8__(*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x10) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0xF) << 56) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0xE) << 48) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0xD) << 40) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0xC) << 32) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0xB) << 24) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0xA) << 16) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 9) << 8) ^ __ROR8__(*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 8) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 7) << 56) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 6) << 48) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 5) << 40) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 4) << 32) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 3) << 24) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 2) << 16) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 1) << 8) ^ __ROR8__(v8 ^ *(unsigned __int8 *)((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL), 57), 57), 57), 57), 57), 57), 57), 57), 57), 57), 57), 57), 57), 57), 57), 57), 57),
           57);
    v10 = __ROR8__(
            ((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x1F) << 56) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x1E) << 48) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x1D) << 40) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x1C) << 32) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x1B) << 24) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x1A) << 16) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x19) << 8) ^ __ROR8__(*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x18) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x17) << 56) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x16) << 48) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x15) << 40) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x14) << 32) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x13) << 24) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x12) << 16) ^ v9, 57), 57), 57), 57), 57), 57), 57), 57), 57), 57), 57), 57), 57),
            57);
    v11 = (((unsigned int)v10 ^ 0xA5A5A5A5) + 4919) ^ ((v10 ^ 0xA5A5A5A5A5A5A5A5LL) + 2 * (v10 ^ 0x5A5A5A5A5A5A5A5ALL));
    v12 = v11 ^ v10;
    v13 = (v12 >> 33) ^ v12 ^ 0x5A5A17C3;
    v14 = (v12 >> 17) ^ ((_DWORD)v12 << 7);
    if ( (v14 & 8) != 0 )
      v15 = v13 + v14;
    else
      v15 = v13 ^ v14;
    if ( (v14 & 0x800) != 0 )
    {
      HIDWORD(v17) = v15;
      LODWORD(v17) = v15;
      v16 = v17 >> 5;
    }
    else
    {
      v16 = v15 ^ ~v14;
    }
    if ( (v14 & 0x80000) != 0 )
      v18 = v16 + 16962;
    else
      v18 = v16 - 4919;
    LODWORD(v19) = (v18 ^ 0xDEADBEEF) + 102;
    v20 = (v18 + 48879) ^ 0x77;
    v21 = v18 & 1;
    if ( v21 )
      v19 = (unsigned int)v19;
    else
      v19 = v20;
    v22 = (v11 ^ (2 * (v21 | (unsigned __int64)(v19 << 32)))) - ((v21 | (unsigned __int64)(v19 << 32)) >> 2) + 8 * v11;
    if ( v22 == TARGET_SLOT )
      v23 = "Correct!";
    else
      v23 = "Wrong!";
    puts(v23);
    return v22 != TARGET_SLOT;
  }
}
```
입력 전체를 64비트 해시로 변환하고 해시값을 시작점으로 사용합니다.
그다음 ECRET_KEY의 32바이트 데이터를 순차적으로 불러오고 __ROR8__을 사용하여 연산을합니다.
그래서 최종적으로 v22 == TARGET_SLOT을 이 되면 corrct가 뜹니다.
그 다음으론 init_target을 보면
```c
unsigned __int64 init_target()
{
  __int64 v0; // x9
  int64x2_t v17; // q7
  int64x2_t v18; // q17
  int64x2_t v19; // q19
  int64x2_t v20; // q20
  int64x2_t v21; // q21
  int64x2_t v22; // q22
  int64x2_t v23; // q23
  int64x2_t v24; // q24
  int8x16_t v26; // q18
  int8x16_t v27; // q10
  int8x16_t v28; // q17
  int8x16_t v29; // q17
  int8x16_t v30; // q1
  int8x16_t v31; // q26
  int8x16_t v33; // q21
  int8x16_t v36; // q21
  int8x16_t v40; // q19
  int8x16_t v42; // q21
  int8x16_t v44; // q23
  int8x16_t v46; // q24
  int8x16_t v50; // q19
  int8x16_t v51; // q17
  int8x16_t v53; // q20
  int8x16_t v55; // q20
  int8x16_t v59; // q19
  int8x16_t v62; // q20
  int8x16_t v65; // q19
  int8x16_t v69; // q17
  int8x16_t v70; // q17
  int8x16_t v72; // q20
  int8x16_t v75; // q20
  int8x16_t v76; // q22
  int8x16_t v80; // q20
  int8x16_t v82; // q20
  int8x16_t v84; // q21
  int8x16_t v89; // q19
  int8x16_t v90; // q17
  int8x16_t v92; // q21
  int8x16_t v94; // q21
  int8x16_t v98; // q19
  int8x16_t v101; // q21
  int8x16_t v104; // q19
  int8x16_t v106; // q2
  int8x16_t v109; // q0
  int8x16_t v110; // q0
  int8x16_t v111; // q19
  int8x16_t v113; // q21
  int8x16_t v115; // q22
  int8x16_t v117; // q23
  int8x16_t v119; // q24
  int8x16_t v120; // q4
  int8x16_t v121; // q13
  int8x16_t v130; // q19
  int8x16_t v131; // q19
  int8x16_t v132; // q21
  int8x16_t v134; // q22
  int8x16_t v136; // q23
  int8x16_t v138; // q23
  int8x16_t v140; // q23
  int8x16_t v141; // q28
  int8x16_t v142; // q23
  int8x16_t v149; // q17
  int8x16_t v152; // q28
  int8x16_t v153; // q28
  int8x16_t v154; // q29
  int8x16_t v156; // q29
  int8x16_t v158; // q29
  int8x16_t v160; // q29
  int8x16_t v162; // q29
  int8x16_t v163; // q5
  int8x16_t v164; // q29
  int8x16_t v171; // q4
  int8x16_t v174; // q5
  int8x16_t v175; // q5
  int8x16_t v178; // q6
  int8x16_t v181; // q6
  int8x16_t v184; // q6
  int8x16_t v187; // q2
  int8x16_t v190; // q3
  int8x16_t v193; // q2
  int8x16_t v194; // q2
  int8x16_t v195; // q3
  int8x16_t v200; // q2
  int8x16_t v203; // q3
  int8x16_t v206; // q3
  int8x16_t v208; // q3
  int8x16_t v212; // q2
  int8x16_t v213; // q2
  int8x16_t v216; // q3
  int8x16_t v218; // q3
  int8x16_t v222; // q2
  int8x16_t v225; // q1
  int8x16_t v227; // q2
  int8x16_t v231; // q1
  int8x16_t v232; // q1
  int8x16_t v235; // q2
  int8x16_t v238; // q2
  int8x16_t v240; // q2
  int8x16_t v244; // q1
  int8x16_t v247; // q2
  int8x16_t v250; // q1
  int8x16_t v251; // q1
  int8x16_t v254; // q2
  int8x16_t v257; // q2
  int8x16_t v260; // q2
  int8x16_t v262; // q2
  int8x16_t v265; // q1
  int8x16_t v269; // q1
  int8x16_t v270; // q1
  int8x16_t v273; // q2
  int8x16_t v275; // q2
  int8x16_t v279; // q4
  int8x16_t v282; // q3
  int8x16_t v285; // q3
  __int64 v293; // x9
  unsigned int v294; // w10
  _BYTE *v295; // x14
  __int64 v296; // x15
  __int64 v297; // x17
  char v298; // w0
  __int64 v299; // x1
  char v300; // w2
  __int64 v301; // x3
  char v302; // w4
  char v303; // w16
  char v304; // w0
  char v305; // w2
  char v306; // w4
  char v307; // w16
  char v308; // w0
  char v309; // w2
  char v310; // w4
  char v311; // w16
  char v312; // w0
  char v313; // w2
  char v314; // w4
  __int64 i; // x9
  __int64 v316; // x9
  unsigned __int64 result; // x0
  unsigned __int64 v366; // t2
  __int64 v367; // t2
  __int64 v368; // x8
  unsigned __int64 v369; // x10
  unsigned __int64 v370; // x8
  int v371; // w11
  int v372; // w8
  int v373; // w11
  int v374; // w11
  unsigned __int64 v375; // t2
  int v376; // w11
  __int64 v377; // x8
  unsigned int v378; // w12
  unsigned int v379; // w11
  int64x2_t v380; // [xsp+10h] [xbp-4C0h]
  int8x16_t v381; // [xsp+60h] [xbp-470h]
  int8x16_t v382; // [xsp+C0h] [xbp-410h]
  int8x16_t v383; // [xsp+E0h] [xbp-3F0h]
  int8x16_t v384; // [xsp+140h] [xbp-390h]
  int8x16_t v385; // [xsp+160h] [xbp-370h]
  int8x16_t v386; // [xsp+1C0h] [xbp-310h]
  int8x16_t v387; // [xsp+1E0h] [xbp-2F0h]
  int64x2_t v388; // [xsp+1F0h] [xbp-2E0h]
  int64x2_t v389; // [xsp+200h] [xbp-2D0h]
  int64x2_t v390; // [xsp+210h] [xbp-2C0h]
  int64x2_t v391; // [xsp+220h] [xbp-2B0h]
  int64x2_t v392; // [xsp+230h] [xbp-2A0h]
  int64x2_t v393; // [xsp+240h] [xbp-290h]
  int64x2_t v394; // [xsp+250h] [xbp-280h]
  __int128 v395; // [xsp+280h] [xbp-250h]
  _OWORD v396[11]; // [xsp+290h] [xbp-240h] BYREF
  _BYTE v397[256]; // [xsp+348h] [xbp-188h]

  v0 = 0LL;
  _X10 = 0xDE7309AA1B44228FLL;
  _X11 = 0x40E25519C7B13A6DLL;
  _W12 = -1;
  __asm
  {
    CRC32CX         W12, W12, X11
    CRC32CX         W12, W12, X10
  }
  _Q0 = vmull_p64(0x40E25519C7B13A6DuLL, 0xDE7309AA1B44228FLL);
  _X10 = 0x91EE2DC3175A5A20LL;
  _X11 = 0xF1BE04673190CE52LL;
  __asm
  {
    CRC32CX         W12, W12, X11
    CRC32CX         W12, W12, X10
  }
  _Q1 = vmull_p64(0xF1BE04673190CE52LL, 0x91EE2DC3175A5A20LL);
  __asm { EOR3            V0.16B, V1.16B, V0.16B, V2.16B }
  v395 = _Q0;
  v17 = (int64x2_t)xmmword_100001DC0;
  v18 = (int64x2_t)xmmword_100001DD0;
  v19 = (int64x2_t)xmmword_100001DE0;
  v20 = (int64x2_t)xmmword_100001DF0;
  v21 = (int64x2_t)xmmword_100001E00;
  v22 = (int64x2_t)xmmword_100001E10;
  v23 = (int64x2_t)xmmword_100001E20;
  v24 = (int64x2_t)xmmword_100001E30;
  _Q16 = (int8x16_t)xmmword_100001E40;
  v26.i64[0] = 0x1B1B1B1B1B1B1B1BLL;
  v26.i64[1] = 0x1B1B1B1B1B1B1B1BLL;
  v380 = vdupq_n_s64(0x10uLL);
  v27.i64[0] = 0x4040404040404040LL;
  v27.i64[1] = 0x4040404040404040LL;
  do
  {
    v388 = v24;
    v389 = v23;
    v390 = v22;
    v391 = v21;
    v392 = v20;
    v393 = v19;
    v394 = v18;
    v28 = vaddq_s8(_Q16, _Q16);
    v29 = vbslq_s8(vcltzq_s8(_Q16), veorq_s8(v28, v26), v28);
    v30.i64[0] = 0x202020202020202LL;
    v30.i64[1] = 0x202020202020202LL;
    v31.i64[0] = 0x202020202020202LL;
    v31.i64[1] = 0x202020202020202LL;
    _Q19 = vbicq_s8(v29, vceqzq_s8(vandq_s8(_Q16, v30)));
    v33 = vaddq_s8(v29, v29);
    _Q17 = vbslq_s8(vcltzq_s8(v29), veorq_s8(v33, v26), v33);
    __asm { BCAX            V0.16B, V19.16B, V16.16B, V0.16B }
    v36 = vaddq_s8(_Q17, _Q17);
    __asm { BCAX            V0.16B, V0.16B, V17.16B, V19.16B }
    _Q17 = vbslq_s8(vcltzq_s8(_Q17), veorq_s8(v36, v26), v36);
    __asm { BCAX            V0.16B, V0.16B, V17.16B, V19.16B }
    v40 = vaddq_s8(_Q17, _Q17);
    _Q17 = vbslq_s8(vcltzq_s8(_Q17), veorq_s8(v40, v26), v40);
    v42 = vaddq_s8(_Q17, _Q17);
    _Q21 = vbslq_s8(vcltzq_s8(_Q17), veorq_s8(v42, v26), v42);
    v44 = vaddq_s8(_Q21, _Q21);
    __asm { BCAX            V0.16B, V0.16B, V17.16B, V19.16B }
    v30.i64[0] = 0x4040404040404040LL;
    v30.i64[1] = 0x4040404040404040LL;
    v46.i64[0] = 0x4040404040404040LL;
    v46.i64[1] = 0x4040404040404040LL;
    __asm { BCAX            V0.16B, V0.16B, V21.16B, V22.16B }
    _Q19 = vbicq_s8(vbslq_s8(vcltzq_s8(_Q21), veorq_s8(v44, v26), v44), vceqzq_s8(vandq_s8(_Q16, v30)));
    __asm { EOR3            V0.16B, V0.16B, V19.16B, V17.16B }
    v50 = vaddq_s8(_Q0, _Q0);
    v51 = vbslq_s8(vcltzq_s8(_Q0), veorq_s8(v50, v26), v50);
    v386 = vceqzq_s8(vandq_s8(_Q0, v31));
    _Q19 = vbicq_s8(v51, v386);
    v53 = vaddq_s8(v51, v51);
    _Q17 = vbslq_s8(vcltzq_s8(v51), veorq_s8(v53, v26), v53);
    v55 = vaddq_s8(_Q17, _Q17);
    __asm { BCAX            V19.16B, V19.16B, V0.16B, V3.16B }
    _Q20 = vbslq_s8(vcltzq_s8(_Q17), veorq_s8(v55, v26), v55);
    __asm { BCAX            V17.16B, V19.16B, V17.16B, V1.16B }
    v59 = vaddq_s8(_Q20, _Q20);
    _Q19 = vbslq_s8(vcltzq_s8(_Q20), veorq_s8(v59, v26), v59);
    __asm { BCAX            V17.16B, V17.16B, V20.16B, V3.16B }
    v62 = vaddq_s8(_Q19, _Q19);
    _Q20 = vbslq_s8(vcltzq_s8(_Q19), veorq_s8(v62, v26), v62);
    __asm { BCAX            V17.16B, V17.16B, V19.16B, V3.16B }
    v65 = vaddq_s8(_Q20, _Q20);
    __asm { BCAX            V17.16B, V17.16B, V20.16B, V3.16B }
    v387 = vceqzq_s8(vandq_s8(_Q0, v46));
    _Q0 = vbicq_s8(vbslq_s8(vcltzq_s8(_Q20), veorq_s8(v65, v26), v65), v387);
    __asm { EOR3            V0.16B, V17.16B, V0.16B, V19.16B }
    v69 = vaddq_s8(_Q0, _Q0);
    v70 = vbslq_s8(vcltzq_s8(_Q0), veorq_s8(v69, v26), v69);
    v385 = vceqzq_s8(vandq_s8(_Q0, v31));
    _Q19 = vbicq_s8(v70, v385);
    v72 = vaddq_s8(v70, v70);
    _Q17 = vbslq_s8(vcltzq_s8(v70), veorq_s8(v72, v26), v72);
    __asm { BCAX            V19.16B, V19.16B, V0.16B, V3.16B }
    v75 = vaddq_s8(_Q17, _Q17);
    v76 = vcltzq_s8(_Q17);
    __asm { BCAX            V17.16B, V19.16B, V17.16B, V3.16B }
    _Q19 = vbslq_s8(v76, veorq_s8(v75, v26), v75);
    __asm { BCAX            V17.16B, V17.16B, V19.16B, V3.16B }
    v80 = vaddq_s8(_Q19, _Q19);
    _Q19 = vbslq_s8(vcltzq_s8(_Q19), veorq_s8(v80, v26), v80);
    v82 = vaddq_s8(_Q19, _Q19);
    _Q20 = vbslq_s8(vcltzq_s8(_Q19), veorq_s8(v82, v26), v82);
    v84 = vaddq_s8(_Q20, _Q20);
    __asm { BCAX            V17.16B, V17.16B, V19.16B, V4.16B }
    v384 = vceqzq_s8(vandq_s8(_Q0, v46));
    __asm { BCAX            V0.16B, V17.16B, V20.16B, V3.16B }
    _Q17 = vbicq_s8(vbslq_s8(vcltzq_s8(_Q20), veorq_s8(v84, v26), v84), v384);
    __asm { EOR3            V0.16B, V0.16B, V17.16B, V19.16B }
    v89 = vaddq_s8(_Q0, _Q0);
    v90 = vbslq_s8(vcltzq_s8(_Q0), veorq_s8(v89, v26), v89);
    v383 = vceqzq_s8(vandq_s8(_Q0, v31));
    _Q19 = vbicq_s8(v90, v383);
    v92 = vaddq_s8(v90, v90);
    _Q17 = vbslq_s8(vcltzq_s8(v90), veorq_s8(v92, v26), v92);
    v94 = vaddq_s8(_Q17, _Q17);
    __asm { BCAX            V19.16B, V19.16B, V0.16B, V2.16B }
    _Q21 = vbslq_s8(vcltzq_s8(_Q17), veorq_s8(v94, v26), v94);
    __asm { BCAX            V17.16B, V19.16B, V17.16B, V3.16B }
    v98 = vaddq_s8(_Q21, _Q21);
    _Q19 = vbslq_s8(vcltzq_s8(_Q21), veorq_s8(v98, v26), v98);
    __asm { BCAX            V17.16B, V17.16B, V21.16B, V2.16B }
    v101 = vaddq_s8(_Q19, _Q19);
    _Q21 = vbslq_s8(vcltzq_s8(_Q19), veorq_s8(v101, v26), v101);
    __asm { BCAX            V17.16B, V17.16B, V19.16B, V1.16B }
    v104 = vaddq_s8(_Q21, _Q21);
    __asm { BCAX            V17.16B, V17.16B, V21.16B, V1.16B }
    v106.i64[0] = 0x4040404040404040LL;
    v106.i64[1] = 0x4040404040404040LL;
    v382 = vceqzq_s8(vandq_s8(_Q0, v106));
    _Q0 = vbicq_s8(vbslq_s8(vcltzq_s8(_Q21), veorq_s8(v104, v26), v104), v382);
    __asm { EOR3            V17.16B, V17.16B, V0.16B, V19.16B }
    v109 = vaddq_s8(_Q17, _Q17);
    v110 = vbslq_s8(vcltzq_s8(_Q17), veorq_s8(v109, v26), v109);
    v111 = vaddq_s8(v110, v110);
    _Q19 = vbslq_s8(vcltzq_s8(v110), veorq_s8(v111, v26), v111);
    v113 = vaddq_s8(_Q19, _Q19);
    _Q21 = vbslq_s8(vcltzq_s8(_Q19), veorq_s8(v113, v26), v113);
    v115 = vaddq_s8(_Q21, _Q21);
    _Q22 = vbslq_s8(vcltzq_s8(_Q21), veorq_s8(v115, v26), v115);
    v117 = vaddq_s8(_Q22, _Q22);
    _Q23 = vbslq_s8(vcltzq_s8(_Q22), veorq_s8(v117, v26), v117);
    v119 = vaddq_s8(_Q23, _Q23);
    v120.i64[0] = 0x202020202020202LL;
    v120.i64[1] = 0x202020202020202LL;
    v121 = vceqzq_s8(vandq_s8(_Q17, v120));
    _Q0 = vbicq_s8(v110, v121);
    __asm
    {
      BCAX            V0.16B, V0.16B, V17.16B, V14.16B
      BCAX            V0.16B, V0.16B, V19.16B, V1.16B
      BCAX            V19.16B, V0.16B, V21.16B, V1.16B
      BCAX            V19.16B, V19.16B, V22.16B, V0.16B
      BCAX            V19.16B, V19.16B, V23.16B, V0.16B
    }
    v381 = vceqzq_s8(vandq_s8(_Q17, v106));
    _Q17 = vbicq_s8(vbslq_s8(vcltzq_s8(_Q23), veorq_s8(v119, v26), v119), v381);
    __asm { EOR3            V17.16B, V19.16B, V17.16B, V21.16B }
    v130 = vaddq_s8(_Q17, _Q17);
    v131 = vbslq_s8(vcltzq_s8(_Q17), veorq_s8(v130, v26), v130);
    v132 = vaddq_s8(v131, v131);
    _Q21 = vbslq_s8(vcltzq_s8(v131), veorq_s8(v132, v26), v132);
    v134 = vaddq_s8(_Q21, _Q21);
    _Q22 = vbslq_s8(vcltzq_s8(_Q21), veorq_s8(v134, v26), v134);
    v136 = vaddq_s8(_Q22, _Q22);
    _Q25 = vbslq_s8(vcltzq_s8(_Q22), veorq_s8(v136, v26), v136);
    v138 = vaddq_s8(_Q25, _Q25);
    _Q11 = vbslq_s8(vcltzq_s8(_Q25), veorq_s8(v138, v26), v138);
    v140 = vaddq_s8(_Q11, _Q11);
    v141 = vbslq_s8(vcltzq_s8(_Q11), veorq_s8(v140, v26), v140);
    _Q0.i64[0] = 0x202020202020202LL;
    _Q0.i64[1] = 0x202020202020202LL;
    v142 = vceqzq_s8(vandq_s8(_Q17, v120));
    _Q19 = vbicq_s8(v131, v142);
    __asm
    {
      BCAX            V19.16B, V19.16B, V17.16B, V27.16B
      BCAX            V19.16B, V19.16B, V21.16B, V24.16B
      BCAX            V22.16B, V19.16B, V22.16B, V21.16B
      BCAX            V22.16B, V22.16B, V25.16B, V19.16B
      BCAX            V22.16B, V22.16B, V11.16B, V25.16B
    }
    v149 = vceqzq_s8(vandq_s8(_Q17, v106));
    _Q28 = vbicq_s8(v141, v149);
    __asm { EOR3            V11.16B, V22.16B, V28.16B, V29.16B }
    v152 = vaddq_s8(_Q11, _Q11);
    v153 = vbslq_s8(vcltzq_s8(_Q11), veorq_s8(v152, v26), v152);
    v154 = vaddq_s8(v153, v153);
    _Q1 = vbslq_s8(vcltzq_s8(v153), veorq_s8(v154, v26), v154);
    v156 = vaddq_s8(_Q1, _Q1);
    _Q2 = vbslq_s8(vcltzq_s8(_Q1), veorq_s8(v156, v26), v156);
    v158 = vaddq_s8(_Q2, _Q2);
    _Q3 = vbslq_s8(vcltzq_s8(_Q2), veorq_s8(v158, v26), v158);
    v160 = vaddq_s8(_Q3, _Q3);
    _Q4 = vbslq_s8(vcltzq_s8(_Q3), veorq_s8(v160, v26), v160);
    v162 = vaddq_s8(_Q4, _Q4);
    v163 = vbslq_s8(vcltzq_s8(_Q4), veorq_s8(v162, v26), v162);
    v164 = vceqzq_s8(vandq_s8(_Q11, _Q0));
    _Q28 = vbicq_s8(v153, v164);
    __asm
    {
      BCAX            V28.16B, V28.16B, V11.16B, V30.16B
      BCAX            V1.16B, V28.16B, V1.16B, V7.16B
      BCAX            V1.16B, V1.16B, V2.16B, V28.16B
      BCAX            V1.16B, V1.16B, V3.16B, V2.16B
      BCAX            V1.16B, V1.16B, V4.16B, V3.16B
    }
    v171 = vceqzq_s8(vandq_s8(_Q11, v27));
    _Q5 = vbicq_s8(v163, v171);
    __asm { EOR3            V1.16B, V1.16B, V5.16B, V6.16B }
    v174 = vaddq_s8(_Q1, _Q1);
    v175 = vbslq_s8(vcltzq_s8(_Q1), veorq_s8(v174, v26), v174);
    _Q6 = vbicq_s8(v175, v164);
    __asm { BCAX            V1.16B, V6.16B, V1.16B, V30.16B }
    v178 = vaddq_s8(v175, v175);
    _Q5 = vbslq_s8(vcltzq_s8(v175), veorq_s8(v178, v26), v178);
    __asm { BCAX            V1.16B, V1.16B, V5.16B, V7.16B }
    v181 = vaddq_s8(_Q5, _Q5);
    _Q5 = vbslq_s8(vcltzq_s8(_Q5), veorq_s8(v181, v26), v181);
    __asm { BCAX            V1.16B, V1.16B, V5.16B, V28.16B }
    v184 = vaddq_s8(_Q5, _Q5);
    _Q5 = vbslq_s8(vcltzq_s8(_Q5), veorq_s8(v184, v26), v184);
    __asm { BCAX            V1.16B, V1.16B, V5.16B, V2.16B }
    v187 = vaddq_s8(_Q5, _Q5);
    _Q2 = vbslq_s8(vcltzq_s8(_Q5), veorq_s8(v187, v26), v187);
    __asm { BCAX            V1.16B, V1.16B, V2.16B, V3.16B }
    v190 = vaddq_s8(_Q2, _Q2);
    _Q2 = vbicq_s8(vbslq_s8(vcltzq_s8(_Q2), veorq_s8(v190, v26), v190), v171);
    __asm { EOR3            V1.16B, V1.16B, V2.16B, V3.16B }
    v193 = vaddq_s8(_Q1, _Q1);
    v194 = vbslq_s8(vcltzq_s8(_Q1), veorq_s8(v193, v26), v193);
    v195 = vaddq_s8(v194, v194);
    _Q3 = vbslq_s8(vcltzq_s8(v194), veorq_s8(v195, v26), v195);
    _Q2 = vbicq_s8(v194, v142);
    __asm
    {
      BCAX            V1.16B, V2.16B, V1.16B, V27.16B
      BCAX            V1.16B, V1.16B, V3.16B, V24.16B
    }
    v200 = vaddq_s8(_Q3, _Q3);
    _Q2 = vbslq_s8(vcltzq_s8(_Q3), veorq_s8(v200, v26), v200);
    __asm { BCAX            V1.16B, V1.16B, V2.16B, V21.16B }
    v203 = vaddq_s8(_Q2, _Q2);
    _Q2 = vbslq_s8(vcltzq_s8(_Q2), veorq_s8(v203, v26), v203);
    __asm { BCAX            V1.16B, V1.16B, V2.16B, V19.16B }
    v206 = vaddq_s8(_Q2, _Q2);
    _Q2 = vbslq_s8(vcltzq_s8(_Q2), veorq_s8(v206, v26), v206);
    v208 = vaddq_s8(_Q2, _Q2);
    __asm { BCAX            V1.16B, V1.16B, V2.16B, V25.16B }
    _Q2 = vbicq_s8(vbslq_s8(vcltzq_s8(_Q2), veorq_s8(v208, v26), v208), v149);
    __asm { EOR3            V1.16B, V1.16B, V2.16B, V3.16B }
    v212 = vaddq_s8(_Q1, _Q1);
    v213 = vbslq_s8(vcltzq_s8(_Q1), veorq_s8(v212, v26), v212);
    _Q3 = vbicq_s8(v213, v121);
    __asm { BCAX            V1.16B, V3.16B, V1.16B, V14.16B }
    v216 = vaddq_s8(v213, v213);
    _Q2 = vbslq_s8(vcltzq_s8(v213), veorq_s8(v216, v26), v216);
    v218 = vaddq_s8(_Q2, _Q2);
    _Q3 = vbslq_s8(vcltzq_s8(_Q2), veorq_s8(v218, v26), v218);
    __asm
    {
      BCAX            V1.16B, V1.16B, V2.16B, V4.16B
      BCAX            V1.16B, V1.16B, V3.16B, V0.16B
    }
    v222 = vaddq_s8(_Q3, _Q3);
    _Q2 = vbslq_s8(vcltzq_s8(_Q3), veorq_s8(v222, v26), v222);
    __asm { BCAX            V0.16B, V1.16B, V2.16B, V0.16B }
    v225 = vaddq_s8(_Q2, _Q2);
    _Q1 = vbslq_s8(vcltzq_s8(_Q2), veorq_s8(v225, v26), v225);
    v227 = vaddq_s8(_Q1, _Q1);
    __asm { BCAX            V0.16B, V0.16B, V1.16B, V3.16B }
    _Q1 = vbicq_s8(vbslq_s8(vcltzq_s8(_Q1), veorq_s8(v227, v26), v227), v381);
    __asm { EOR3            V0.16B, V0.16B, V1.16B, V2.16B }
    v231 = vaddq_s8(_Q0, _Q0);
    v232 = vbslq_s8(vcltzq_s8(_Q0), veorq_s8(v231, v26), v231);
    _Q2 = vbicq_s8(v232, v383);
    __asm { BCAX            V0.16B, V2.16B, V0.16B, V3.16B }
    v235 = vaddq_s8(v232, v232);
    _Q1 = vbslq_s8(vcltzq_s8(v232), veorq_s8(v235, v26), v235);
    __asm { BCAX            V0.16B, V0.16B, V1.16B, V2.16B }
    v238 = vaddq_s8(_Q1, _Q1);
    _Q1 = vbslq_s8(vcltzq_s8(_Q1), veorq_s8(v238, v26), v238);
    v240 = vaddq_s8(_Q1, _Q1);
    _Q2 = vbslq_s8(vcltzq_s8(_Q1), veorq_s8(v240, v26), v240);
    __asm
    {
      BCAX            V0.16B, V0.16B, V1.16B, V3.16B
      BCAX            V0.16B, V0.16B, V2.16B, V1.16B
    }
    v244 = vaddq_s8(_Q2, _Q2);
    _Q1 = vbslq_s8(vcltzq_s8(_Q2), veorq_s8(v244, v26), v244);
    __asm { BCAX            V0.16B, V0.16B, V1.16B, V2.16B }
    v247 = vaddq_s8(_Q1, _Q1);
    _Q1 = vbicq_s8(vbslq_s8(vcltzq_s8(_Q1), veorq_s8(v247, v26), v247), v382);
    __asm { EOR3            V0.16B, V0.16B, V1.16B, V2.16B }
    v250 = vaddq_s8(_Q0, _Q0);
    v251 = vbslq_s8(vcltzq_s8(_Q0), veorq_s8(v250, v26), v250);
    _Q2 = vbicq_s8(v251, v385);
    __asm { BCAX            V0.16B, V2.16B, V0.16B, V3.16B }
    v254 = vaddq_s8(v251, v251);
    _Q1 = vbslq_s8(vcltzq_s8(v251), veorq_s8(v254, v26), v254);
    __asm { BCAX            V0.16B, V0.16B, V1.16B, V2.16B }
    v257 = vaddq_s8(_Q1, _Q1);
    _Q1 = vbslq_s8(vcltzq_s8(_Q1), veorq_s8(v257, v26), v257);
    __asm { BCAX            V0.16B, V0.16B, V1.16B, V2.16B }
    v260 = vaddq_s8(_Q1, _Q1);
    _Q1 = vbslq_s8(vcltzq_s8(_Q1), veorq_s8(v260, v26), v260);
    v262 = vaddq_s8(_Q1, _Q1);
    _Q2 = vbslq_s8(vcltzq_s8(_Q1), veorq_s8(v262, v26), v262);
    __asm { BCAX            V0.16B, V0.16B, V1.16B, V3.16B }
    v265 = vaddq_s8(_Q2, _Q2);
    __asm { BCAX            V0.16B, V0.16B, V2.16B, V4.16B }
    _Q1 = vbicq_s8(vbslq_s8(vcltzq_s8(_Q2), veorq_s8(v265, v26), v265), v384);
    __asm { EOR3            V0.16B, V0.16B, V1.16B, V2.16B }
    v269 = vaddq_s8(_Q0, _Q0);
    v270 = vbslq_s8(vcltzq_s8(_Q0), veorq_s8(v269, v26), v269);
    _Q2 = vbicq_s8(v270, v386);
    __asm { BCAX            V0.16B, V2.16B, V0.16B, V3.16B }
    v273 = vaddq_s8(v270, v270);
    _Q1 = vbslq_s8(vcltzq_s8(v270), veorq_s8(v273, v26), v273);
    v275 = vaddq_s8(_Q1, _Q1);
    _Q2 = vbslq_s8(vcltzq_s8(_Q1), veorq_s8(v275, v26), v275);
    __asm
    {
      BCAX            V0.16B, V0.16B, V1.16B, V4.16B
      BCAX            V0.16B, V0.16B, V2.16B, V4.16B
    }
    v279 = vaddq_s8(_Q2, _Q2);
    _Q2 = vbslq_s8(vcltzq_s8(_Q2), veorq_s8(v279, v26), v279);
    __asm { BCAX            V0.16B, V0.16B, V2.16B, V3.16B }
    v282 = vaddq_s8(_Q2, _Q2);
    _Q2 = vbslq_s8(vcltzq_s8(_Q2), veorq_s8(v282, v26), v282);
    __asm { BCAX            V0.16B, V0.16B, V2.16B, V3.16B }
    v285 = vaddq_s8(_Q2, _Q2);
    _Q2 = vbicq_s8(vbslq_s8(vcltzq_s8(_Q2), veorq_s8(v285, v26), v285), v387);
    __asm { EOR3            V0.16B, V0.16B, V2.16B, V3.16B }
    _Q0 = vbicq_s8(
            _Q0,
            vuzp1q_s8(
              (int8x16_t)vuzp1q_s16(
                           (int16x8_t)vuzp1q_s32((int32x4_t)vceqzq_s64(v388), (int32x4_t)vceqzq_s64(v389)),
                           (int16x8_t)vuzp1q_s32((int32x4_t)vceqzq_s64(v390), (int32x4_t)vceqzq_s64(v391))),
              (int8x16_t)vuzp1q_s16(
                           (int16x8_t)vuzp1q_s32((int32x4_t)vceqzq_s64(v392), (int32x4_t)vceqzq_s64(v393)),
                           (int16x8_t)vuzp1q_s32((int32x4_t)vceqzq_s64(v394), (int32x4_t)vceqzq_s64(v17)))));
    _Q3 = vsraq_n_u8((uint8x16_t)vshlq_n_s8(_Q0, 3uLL), (uint8x16_t)_Q0, 5uLL);
    _Q1 = veorq_s8(
            (int8x16_t)vsraq_n_u8((uint8x16_t)vaddq_s8(_Q0, _Q0), (uint8x16_t)_Q0, 7uLL),
            (int8x16_t)vsraq_n_u8((uint8x16_t)vshlq_n_s8(_Q0, 2uLL), (uint8x16_t)_Q0, 6uLL));
    __asm
    {
      EOR3            V1.16B, V1.16B, V3.16B, V2.16B
      EOR3            V0.16B, V1.16B, V0.16B, V2.16B
    }
    *(int8x16_t *)&v397[v0] = _Q0;
    v22 = vaddq_s64(v390, v380);
    v23 = vaddq_s64(v389, v380);
    v24 = vaddq_s64(v388, v380);
    v21 = vaddq_s64(v391, v380);
    v0 += 16LL;
    v20 = vaddq_s64(v392, v380);
    v19 = vaddq_s64(v393, v380);
    v18 = vaddq_s64(v394, v380);
    v17 = vaddq_s64(v17, v380);
    _Q0.i64[0] = 0x1010101010101010LL;
    _Q0.i64[1] = 0x1010101010101010LL;
    _Q16 = vaddq_s8(_Q16, _Q0);
  }
  while ( v0 != 256 );
  v293 = 0LL;
  v396[0] = v395;
  v294 = 1;
  do
  {
    v295 = &v396[v293];
    v296 = BYTE13(v396[v293]);
    v297 = BYTE14(v396[v293]);
    v298 = v397[v297];
    v299 = HIBYTE(v396[v293]);
    v300 = v397[v299];
    v301 = BYTE12(v396[v293]);
    v302 = v397[v301];
    v303 = v397[v296] ^ LOBYTE(v396[v293]) ^ v294;
    v295[16] = v303;
    v304 = BYTE1(v396[v293]) ^ v298;
    v295[17] = v304;
    v305 = BYTE2(v396[v293]) ^ v300;
    v295[18] = v305;
    v306 = BYTE3(v396[v293]) ^ v302;
    v295[19] = v306;
    v307 = BYTE4(v396[v293]) ^ v303;
    v295[20] = v307;
    v308 = BYTE5(v396[v293]) ^ v304;
    v295[21] = v308;
    v309 = BYTE6(v396[v293]) ^ v305;
    v295[22] = v309;
    v310 = BYTE7(v396[v293]) ^ v306;
    v295[23] = v310;
    v311 = BYTE8(v396[v293]) ^ v307;
    v295[24] = v311;
    v312 = BYTE9(v396[v293]) ^ v308;
    v295[25] = v312;
    v313 = BYTE10(v396[v293]) ^ v309;
    v295[26] = v313;
    v314 = BYTE11(v396[v293]) ^ v310;
    v295[27] = v314;
    v295[28] = v311 ^ v301;
    v295[29] = v312 ^ v296;
    v295[30] = v313 ^ v297;
    if ( (_BYTE)v294 )
      v294 = ((unsigned int)(char)v294 >> 7) & 0x1B ^ (2 * v294);
    else
      v294 = 1;
    ++v293;
    v295[31] = v314 ^ v299;
  }
  while ( v293 != 10 );
  for ( i = 0LL; i != 256; ++i )
    v397[i] = 0;
  v316 = 0LL;
  _Q18 = v396[1];
  _Q17 = v396[2];
  _Q16 = v396[3];
  _Q7 = v396[4];
  _Q6 = v396[5];
  _Q5 = v396[6];
  _Q4 = v396[7];
  _Q3 = v396[8];
  _Q2 = v396[9];
  _Q1 = v396[10];
  __asm
  {
    AESE            V0.16B, V18.16B
    AESMC           V0.16B, V0.16B
    AESE            V0.16B, V17.16B
    AESMC           V0.16B, V0.16B
    AESE            V0.16B, V16.16B
    AESMC           V0.16B, V0.16B
    AESE            V0.16B, V7.16B
    AESMC           V0.16B, V0.16B
    AESE            V0.16B, V6.16B
    AESMC           V0.16B, V0.16B
    AESE            V0.16B, V5.16B
    AESMC           V0.16B, V0.16B
    AESE            V0.16B, V4.16B
    AESMC           V0.16B, V0.16B
    AESE            V0.16B, V3.16B
    AESMC           V0.16B, V0.16B
    AESE            V0.16B, V2.16B
    AESMC           V0.16B, V0.16B
    AESE            V0.16B, V1.16B
    AESE            V19.16B, V18.16B
    AESMC           V19.16B, V19.16B
    AESE            V19.16B, V17.16B
    AESMC           V19.16B, V19.16B
    AESE            V19.16B, V16.16B
    AESMC           V19.16B, V19.16B
    AESE            V19.16B, V7.16B
    AESMC           V19.16B, V19.16B
    AESE            V19.16B, V6.16B
    AESMC           V19.16B, V19.16B
    AESE            V19.16B, V5.16B
    AESMC           V19.16B, V19.16B
    AESE            V19.16B, V4.16B
    AESMC           V19.16B, V19.16B
    AESE            V19.16B, V3.16B
    AESMC           V19.16B, V19.16B
    AESE            V19.16B, V2.16B
    AESMC           V19.16B, V19.16B
    AESE            V19.16B, V1.16B
  }
  result = 0x100000001B3LL
         * ((0x100000001B3LL
           * ((0x100000001B3LL
             * ((0x100000001B3LL
               * ((0x100000001B3LL
                 * ((0x100000001B3LL
                   * ((0x100000001B3LL * ((unsigned __int8)_Q0 ^ 0xCBF29CE484222369LL)) ^ BYTE1(_Q0) ^ 0x16u)) ^ BYTE2(_Q0) ^ 0xADu)) ^ BYTE3(_Q0) ^ 0xB9u)) ^ BYTE4(_Q0) ^ 0x83u)) ^ BYTE5(_Q0) ^ 0x3Eu)) ^ BYTE6(_Q0) ^ 0x71u);
  v366 = __ROR8__(
           ((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 7) << 56) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 6) << 48) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 5) << 40) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 4) << 32) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 3) << 24) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 2) << 16) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 1) << 8) ^ __ROR8__((0x100000001B3LL * ((0x100000001B3LL * ((0x100000001B3LL * ((0x100000001B3LL * ((0x100000001B3LL * ((0x100000001B3LL * ((0x100000001B3LL * ((0x100000001B3LL * ((0x100000001B3LL * ((0x100000001B3LL * ((0x100000001B3LL * ((0x100000001B3LL * ((0x100000001B3LL * ((0x100000001B3LL * ((0x100000001B3LL * ((0x100000001B3LL * ((0x100000001B3LL * (result ^ (unsigned __int8)~BYTE7(_Q0))) ^ BYTE8(_Q0) ^ 0xE4u)) ^ BYTE9(_Q0) ^ 0xE5u)) ^ BYTE10(_Q0) ^ 0x3Au)) ^ BYTE11(_Q0) ^ 0x52u)) ^ BYTE12(_Q0) ^ 9u)) ^ BYTE13(_Q0) ^ 0x80u)) ^ BYTE14(_Q0) ^ 0x46u)) ^ HIBYTE(_Q0) ^ 0x54u)) ^ (unsigned __int8)_Q19 ^ 0xF5u)) ^ BYTE1(_Q19) ^ 0x4Au)) ^ BYTE2(_Q19) ^ 0xC3u)) ^ BYTE3(_Q19) ^ 0x59u)) ^ BYTE4(_Q19) ^ 0x3Du)) ^ BYTE5(_Q19) ^ 0xA7u)) ^ BYTE6(_Q19) ^ 0x10u)) ^ BYTE7(_Q19) ^ 0x70u)) ^ *(unsigned __int8 *)((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL), 57), 57), 57), 57), 57), 57), 57),
           57);
  v367 = __ROR8__(
           ((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x19) << 8) ^ __ROR8__(*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x18) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x17) << 56) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x16) << 48) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x15) << 40) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x14) << 32) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x13) << 24) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x12) << 16) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x11) << 8) ^ __ROR8__(*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x10) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0xF) << 56) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0xE) << 48) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0xD) << 40) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0xC) << 32) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0xB) << 24) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0xA) << 16) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 9) << 8) ^ __ROR8__(*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 8) ^ v366, 57), 57), 57), 57), 57), 57), 57), 57), 57), 57), 57), 57), 57), 57), 57), 57), 57),
           57);
  v368 = __ROR8__(
           ((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x1F) << 56) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x1E) << 48) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x1D) << 40) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x1C) << 32) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x1B) << 24) ^ __ROR8__(((unsigned __int64)*(unsigned __int8 *)(((unsigned __int64)&SECRET_KEY & 0xFFFFFFFFFFFFFFLL) + 0x1A) << 16) ^ v367, 57), 57), 57), 57), 57),
           57);
  v369 = (((unsigned int)v368 ^ 0xA5A5A5A5) + 4919) ^ ((v368 ^ 0xA5A5A5A5A5A5A5A5LL) + 2 * (v368 ^ 0x5A5A5A5A5A5A5A5ALL));
  v370 = v369 ^ v368;
  v371 = (v370 >> 33) ^ v370 ^ 0x5A5A17C3;
  v372 = (v370 >> 17) ^ ((_DWORD)v370 << 7);
  if ( (v372 & 8) != 0 )
    v373 = v371 + v372;
  else
    v373 = v371 ^ v372;
  if ( (v372 & 0x800) != 0 )
  {
    HIDWORD(v375) = v373;
    LODWORD(v375) = v373;
    v374 = v375 >> 5;
  }
  else
  {
    v374 = v373 ^ ~v372;
  }
  if ( (v372 & 0x80000) != 0 )
    v376 = v374 + 16962;
  else
    v376 = v374 - 4919;
  LODWORD(v377) = (v376 ^ 0xDEADBEEF) + 102;
  v378 = (v376 + 48879) ^ 0x77;
  v379 = v376 & 1;
  if ( v379 )
    v377 = (unsigned int)v377;
  else
    v377 = v378;
  TARGET_SLOT = (v369 ^ (2 * (v379 | (unsigned __int64)(v377 << 32))))
              - ((v379 | (unsigned __int64)(v377 << 32)) >> 2)
              + 8 * v369;
  v397[0] = 0;
  do
    *((_BYTE *)v396 + v316++) = 0;
  while ( v316 != 176 );
  return result;
}
```
코드를 보면 init_target()를 호출하고 내부 동작을 통해
AES S-box를 동적으로 생성해 256바이트 테이블을 만들고
그 S-box로 AES-128 라운드키 10개를 계산한 뒤 위 두 블록의 바이트를 해시로 바꾼다
그리고 SECRET_KEY를 통해 연산해서 TARGET_SLOT을 결정한다
그리고 또한 마지막에 S-box와 라운드키를 지워서 동적으로 값을 확인해야 한다.
그래서 메모리에서 값을 읽고 역연산 하면되는데 ARMv8 NEON/crypto 인스트럭션이 많이 구현되어 있어서 
python 같은 언어로는 역연산하기 상당히 까다롭다
따라서 arm_neon.h와 같은 arm 헤더가 있는 c언어로 역연산을 작성한다면 비교적 쉽게 역연산을 작성할 수 있습니다.
```c
// gcc -O2 -march=armv8-a+crypto+crc -o solve solve.c
#include <stdio.h>
#include <arm_neon.h>
#include <arm_acle.h>
#include <stdint.h>
#include <string.h>


static const uint8_t SECRET_KEY[32] = {
    0x6D,0x3A,0xB1,0xC7,0x19,0x55,0xE2,0x40,
    0x8F,0x22,0x44,0x1B,0xAA,0x09,0x73,0xDE,
    0x52,0xCE,0x90,0x31,0x67,0x04,0xBE,0xF1,
    0x20,0x5A,0x5A,0x17,0xC3,0x2D,0xEE,0x91
};

static const uint8_t FLAG_NONCE[16] = {
    0x50,0x36,0x70,0x9A,0x2D,0x13,0x8C,0xB6,
    0x5A,0xCC,0xA7,0x21,0xEF,0x10,0x94,0x4D
};
static const uint8_t FLAG_CIPH[24] = {
    0x4C,0x16,0xAD,0xB9,0x83,0x3E,0x71,0xFF,
    0xE4,0xE5,0x3A,0x52,0x09,0x80,0x46,0x54,
    0xF5,0x4A,0xC3,0x59,0x3D,0xA7,0x10,0x70
};
static const unsigned FLAG_LEN = 24;

static const uint8_t SBOX[256] = {
  0x63,0x7c,0x77,0x7b,0xf2,0x6b,0x6f,0xc5,0x30,0x01,0x67,0x2b,0xfe,0xd7,0xab,0x76,
  0xca,0x82,0xc9,0x7d,0xfa,0x59,0x47,0xf0,0xad,0xd4,0xa2,0xaf,0x9c,0xa4,0x72,0xc0,
  0xb7,0xfd,0x93,0x26,0x36,0x3f,0xf7,0xcc,0x34,0xa5,0xe5,0xf1,0x71,0xd8,0x31,0x15,
  0x04,0xc7,0x23,0xc3,0x18,0x96,0x05,0x9a,0x07,0x12,0x80,0xe2,0xeb,0x27,0xb2,0x75,
  0x09,0x83,0x2c,0x1a,0x1b,0x6e,0x5a,0xa0,0x52,0x3b,0xd6,0xb3,0x29,0xe3,0x2f,0x84,
  0x53,0xd1,0x00,0xed,0x20,0xfc,0xb1,0x5b,0x6a,0xcb,0xbe,0x39,0x4a,0x4c,0x58,0xcf,
  0xd0,0xef,0xaa,0xfb,0x43,0x4d,0x33,0x85,0x45,0xf9,0x02,0x7f,0x50,0x3c,0x9f,0xa8,
  0x51,0xa3,0x40,0x8f,0x92,0x9d,0x38,0xf5,0xbc,0xb6,0xda,0x21,0x10,0xff,0xf3,0xd2,
  0xcd,0x0c,0x13,0xec,0x5f,0x97,0x44,0x17,0xc4,0xa7,0x7e,0x3d,0x64,0x5d,0x19,0x73,
  0x60,0x81,0x4f,0xdc,0x22,0x2a,0x90,0x88,0x46,0xee,0xb8,0x14,0xde,0x5e,0x0b,0xdb,
  0xe0,0x32,0x3a,0x0a,0x49,0x06,0x24,0x5c,0xc2,0xd3,0xac,0x62,0x91,0x95,0xe4,0x79,
  0xe7,0xc8,0x37,0x6d,0x8d,0xd5,0x4e,0xa9,0x6c,0x56,0xf4,0xea,0x65,0x7a,0xae,0x08,
  0xba,0x78,0x25,0x2e,0x1c,0xa6,0xb4,0xc6,0xe8,0xdd,0x74,0x1f,0x4b,0xbd,0x8b,0x8a,
  0x70,0x3e,0xb5,0x66,0x48,0x03,0xf6,0x0e,0x61,0x35,0x57,0xb9,0x86,0xc1,0x1d,0x9e,
  0xe1,0xf8,0x98,0x11,0x69,0xd9,0x8e,0x94,0x9b,0x1e,0x87,0xe9,0xce,0x55,0x28,0xdf,
  0x8c,0xa1,0x89,0x0d,0xbf,0xe6,0x42,0x68,0x41,0x99,0x2d,0x0f,0xb0,0x54,0xbb,0x16
};
static inline uint8_t xtime(uint8_t x){ return (uint8_t)((x<<1) ^ ((x>>7)*0x1B)); }

static void aes128_key_expand(const uint8_t key[16], uint8_t rk[11][16]) {
    memcpy(rk[0], key, 16);
    uint8_t rcon = 1;
    for (int i = 1; i <= 10; i++) {
        uint8_t *prev = rk[i-1];
        uint8_t *curr = rk[i];
        uint8_t t0 = prev[13], t1 = prev[14], t2 = prev[15], t3 = prev[12];
        t0 = SBOX[t0]; t1 = SBOX[t1]; t2 = SBOX[t2]; t3 = SBOX[t3];
        t0 ^= rcon; rcon = xtime(rcon);
        curr[0]  = prev[0]  ^ t0;
        curr[1]  = prev[1]  ^ t1;
        curr[2]  = prev[2]  ^ t2;
        curr[3]  = prev[3]  ^ t3;
        for (int j = 4; j < 16; j++) curr[j] = prev[j] ^ curr[j-4];
    }
}

static void aes128_enc_block_arm(const uint8_t in[16], uint8_t out[16], const uint8x16_t rk[11]){
    uint8x16_t b = vld1q_u8(in);
    b = veorq_u8(b, rk[0]);
    for (int i=1;i<10;i++){
        b = vaeseq_u8(b, rk[i]);
        b = vaesmcq_u8(b);
    }
    b = vaeseq_u8(b, rk[10]);
    vst1q_u8(out, b);
}

static void aes128_ctr_xor(uint8_t *dst, const uint8_t *src, size_t n,
                           const uint8_t nonce[16], const uint8x16_t rk[11]){
    uint8_t ctr_blk[16]; memcpy(ctr_blk, nonce, 16);
    uint64_t ctr = 0;
    while (n){
        for (int i=0;i<8;i++) ctr_blk[8+i] = (ctr>>(8*i)) & 0xFF;
        ctr++;
        uint8_t ks[16];
        aes128_enc_block_arm(ctr_blk, ks, rk);
        size_t m = n<16?n:16;
        for (size_t i=0;i<m;i++) dst[i] = src[i] ^ ks[i];
        dst += m; src += m; n -= m;
    }
}

static void derive_k0(uint8_t out16[16]){
    const uint64_t *p = (const uint64_t*)SECRET_KEY;
    poly128_t m1 = vmull_p64((poly64_t)p[0], (poly64_t)p[1]);
    poly128_t m2 = vmull_p64((poly64_t)p[2], (poly64_t)p[3]);
    uint8x16_t x = veorq_u8(vreinterpretq_u8_p128(m1), vreinterpretq_u8_p128(m2));
    uint32_t crc = 0xFFFFFFFFu;
    crc = __crc32cd(crc, p[0]); crc = __crc32cd(crc, p[1]);
    crc = __crc32cd(crc, p[2]); crc = __crc32cd(crc, p[3]);
    uint8x16_t c = vdupq_n_u8((uint8_t)(crc & 0xFF));
    x = veorq_u8(x, c);
    vst1q_u8(out16, x);
}

int main(){
    uint8_t k0[16];
    derive_k0(k0);

    uint8_t rkb[11][16]; aes128_key_expand(k0, rkb);
    uint8x16_t rk[11]; for (int i=0;i<11;i++) rk[i] = vld1q_u8(rkb[i]);

    uint8_t pt[128]={0};
    aes128_ctr_xor(pt, FLAG_CIPH, FLAG_LEN, FLAG_NONCE, rk);

    printf("%.*s\n", FLAG_LEN, pt);
    return 0;
}

```